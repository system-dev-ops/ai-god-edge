{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ai_god_multimodal_5in1_20250723_163629.ipynb",
   "private_outputs": true,
   "provenance": [],
   "uploaded_time": "2024-07-23T09:36:29.800Z"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñ AI God Multimodal 5-in-1 - Google Colab\n",
    "Notebook ‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏ß‡πâ 5 ‡πÅ‡∏ö‡∏ö:\n",
    "\n",
    "üß† Mistral 7B (Text Generation)\n",
    "üß† Phi-2 (‡πÄ‡∏ö‡∏≤‡πÄ‡∏£‡πá‡∏ß)\n",
    "üß† TinyLlama (‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å / ‡∏™‡∏≥‡∏£‡∏≠‡∏á)\n",
    "üé§ Whisper (‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏π‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)\n",
    "üé® Stable Diffusion (‡∏ß‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)\n",
    "‚úÖ ‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö UI ‡∏î‡πâ‡∏ß‡∏¢ Gradio + ‡∏õ‡∏∏‡πà‡∏°‡∏û‡∏¥‡πÄ‡∏®‡∏© + ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏π‡∏î + ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7c65b53-0663-441f-8255-e87f6291a826"
   },
   "outputs": [],
   "source": [
    "# üì¶ STEP 1: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
    "# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô Gradio ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞‡πÑ‡∏î‡πâ\n",
    "# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á numpy ‡πÅ‡∏•‡∏∞ websockets ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ dependency conflicts\n",
    "!pip install -q llama-cpp-python==0.2.64 gradio==3.48.0 transformers accelerate torchaudio diffusers openai-whisper numpy==1.24.4 websockets==11.0.3\n",
    "!apt install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1c8f1a1-3705-4f40-84c4-72b1265f24f0"
   },
   "outputs": [],
   "source": [
    "# üìÅ STEP 2: ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ)\n",
    "# ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå .gguf ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå 'models'\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "HF_MODELS = {\n",
    "    'mistral': {\n",
    "        'repo_id': 'TheBloke/Mistral-7B-Instruct-v0.1-GGUF',\n",
    "        'filename': 'mistral-7b-instruct-v0.1.Q4_K_M.gguf'\n",
    "    },\n",
    "    'phi2': {\n",
    "        'repo_id': 'TheBloke/phi-2-GGUF',\n",
    "        'filename': 'phi-2.Q4_K_M.gguf'\n",
    "    },\n",
    "    'tinyllama': {\n",
    "        'repo_id': 'TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF',\n",
    "        'filename': 'tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf' # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå TinyLlama\n",
    "    }\n",
    "}\n",
    "MODEL_DIR = 'models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Downloading models... This may take some time.\")\n",
    "\n",
    "# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Mistral (‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏∑‡πà‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ)\n",
    "# ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Phi-2 ‡∏´‡∏£‡∏∑‡∏≠ TinyLlama ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤\n",
    "# ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏Ñ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å\n",
    "# Mistral 7B ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô\n",
    "hf_hub_download(\n",
    "    repo_id=HF_MODELS['mistral']['repo_id'],\n",
    "    filename=HF_MODELS['mistral']['filename'],\n",
    "    local_dir=MODEL_DIR,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "# ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Phi-2 ‡∏î‡πâ‡∏ß‡∏¢ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ\n",
    "# hf_hub_download(\n",
    "#     repo_id=HF_MODELS['phi2']['repo_id'],\n",
    "#     filename=HF_MODELS['phi2']['filename'],\n",
    "#     local_dir=MODEL_DIR,\n",
    "#     local_dir_use_symlinks=False\n",
    "# )\n",
    "\n",
    "# ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î TinyLlama ‡∏î‡πâ‡∏ß‡∏¢ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ\n",
    "# hf_hub_download(\n",
    "#     repo_id=HF_MODELS['tinyllama']['repo_id'],\n",
    "#     filename=HF_MODELS['tinyllama']['filename'],\n",
    "#     local_dir=MODEL_DIR,\n",
    "#     local_dir_use_symlinks=False\n",
    "# )\n",
    "\n",
    "print(\"Model download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1f7b0e5-7e04-4c8d-8a43-8557b4f53517"
   },
   "outputs": [],
   "source": [
    "# üß† STEP 3: ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LLM\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
    "mistral_model_path = f\"{MODEL_DIR}/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "# phi2_model_path = f\"{MODEL_DIR}/phi-2.Q4_K_M.gguf\" # ‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏´‡∏≤‡∏Å‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Phi-2\n",
    "# tinyllama_model_path = f\"{MODEL_DIR}/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\" # ‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏´‡∏≤‡∏Å‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î TinyLlama\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î Mistral (‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ô STEP 2 ‡πÅ‡∏•‡πâ‡∏ß)\n",
    "try:\n",
    "    mistral = Llama(model_path=mistral_model_path, n_ctx=2048, n_gpu_layers=-1, verbose=False) # n_gpu_layers=-1 ‡πÉ‡∏ä‡πâ GPU ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    print(\"Mistral model loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Mistral model not found at {mistral_model_path}. Please ensure it was downloaded in STEP 2.\")\n",
    "    mistral = None # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô None ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏´‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î Phi-2 (‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏ô STEP 2 ‡πÅ‡∏•‡πâ‡∏ß)\n",
    "# try:\n",
    "#     phi2 = Llama(model_path=phi2_model_path, n_ctx=2048, n_gpu_layers=-1, verbose=False)\n",
    "#     print(\"Phi-2 model loaded.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Phi-2 model not found at {phi2_model_path}. Please ensure it was downloaded in STEP 2.\")\n",
    "#     phi2 = None\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î TinyLlama (‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏ô STEP 2 ‡πÅ‡∏•‡πâ‡∏ß)\n",
    "# try:\n",
    "#     tinyllama = Llama(model_path=tinyllama_model_path, n_ctx=1024, n_gpu_layers=-1, verbose=False)\n",
    "#     print(\"TinyLlama model loaded.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"TinyLlama model not found at {tinyllama_model_path}. Please ensure it was downloaded in STEP 2.\")\n",
    "#     tinyllama = None\n",
    "\n",
    "\n",
    "def choose_model(name):\n",
    "    # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô choose_model ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
    "    if name == 'Mistral' and ('mistral' in locals() and mistral):\n",
    "        return mistral\n",
    "    elif name == 'Phi-2' and ('phi2' in locals() and phi2): # ‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡πâ Phi-2\n",
    "        return phi2\n",
    "    elif name == 'TinyLlama' and ('tinyllama' in locals() and tinyllama): # ‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡πâ TinyLlama\n",
    "        return tinyllama\n",
    "    else:\n",
    "        print(f\"Warning: Model '{name}' not loaded or not available. Defaulting to Mistral (if available).\")\n",
    "        return mistral if ('mistral' in locals() and mistral) else None # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Mistral ‡∏´‡∏≤‡∏Å‡∏°‡∏µ ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5e9059e-a1b7-4a00-9856-c73f324e933d"
   },
   "outputs": [],
   "source": [
    "# üé§ STEP 4: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏π‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "import whisper\n",
    "asr = whisper.load_model(\"base\") # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î \"base\" ‡∏´‡∏£‡∏∑‡∏≠ \"small\" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß\n",
    "\n",
    "def transcribe(audio):\n",
    "    if audio is None:\n",
    "        return \"No audio provided.\"\n",
    "    try:\n",
    "        # Colab ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏≠‡∏≤‡∏à‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå .webm, whisper ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ .flac, .wav, .mp3, etc.\n",
    "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ whisper ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
    "        # ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà whisper ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà Gradio ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á\n",
    "        return asr.transcribe(audio)[\"text\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error transcribing audio: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8e19c30-1011-404c-a320-b3a164c4805e"
   },
   "outputs": [],
   "source": [
    "# üé® STEP 5: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ GPU ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
    "if torch.cuda.is_available():\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
    "    pipe.to(\"cuda\")\n",
    "    print(\"Stable Diffusion loaded on CUDA.\")\n",
    "else:\n",
    "    print(\"CUDA not available. Stable Diffusion will run on CPU, which might be very slow.\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\") # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ float16 ‡∏ö‡∏ô CPU\n",
    "    pipe.to(\"cpu\")\n",
    "\n",
    "def generate_image(prompt):\n",
    "    if not prompt:\n",
    "        return None # ‡πÑ‡∏°‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ prompt\n",
    "    try:\n",
    "        image = pipe(prompt).images[0]\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1f7b0e5-7e04-4c8d-8a43-8557b4f53517"
   },
   "outputs": [],
   "source": [
    "# üß†üñºÔ∏èüé§ STEP 6: Gradio UI\n",
    "import gradio as gr\n",
    "\n",
    "def chat(text, model_choice):\n",
    "    if not text:\n",
    "        return \"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\"\n",
    "\n",
    "    llm = choose_model(model_choice)\n",
    "    if llm is None:\n",
    "        return f\"‡πÇ‡∏°‡πÄ‡∏î‡∏• {model_choice} ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•\"\n",
    "\n",
    "    try:\n",
    "        # Llama.cpp prompt format ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mistral Instruct\n",
    "        # ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• LLM ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ\n",
    "        if model_choice == \"Mistral\":\n",
    "            formatted_prompt = f\"<s>[INST] {text} [/INST]\"\n",
    "        else: # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Phi-2, TinyLlama ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÜ\n",
    "            formatted_prompt = text\n",
    "\n",
    "        result = llm(formatted_prompt, max_tokens=500)['choices'][0]['text']\n",
    "        return result.strip()\n",
    "    except Exception as e:\n",
    "        return f\"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üîÆ AI God - Multimodal 5-in-1\")\n",
    "    with gr.Row():\n",
    "        # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
    "        available_models = []\n",
    "        if 'mistral' in locals() and mistral:\n",
    "            available_models.append(\"Mistral\")\n",
    "        if 'phi2' in locals() and phi2:\n",
    "            available_models.append(\"Phi-2\")\n",
    "        if 'tinyllama' in locals() and tinyllama:\n",
    "            available_models.append(\"TinyLlama\")\n",
    "\n",
    "        if not available_models:\n",
    "            gr.Markdown(\"**<span style='color:red'>! ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏• LLM ‡πÉ‡∏î‡πÜ ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö STEP 2 ‡πÅ‡∏•‡∏∞ STEP 3. !</span>**\")\n",
    "            model_choice = gr.Dropdown(choices=[], value=None, label=\"üß† ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)\", interactive=False)\n",
    "        else:\n",
    "            model_choice = gr.Dropdown(choices=available_models, value=available_models[0], label=\"üß† ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\")\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(label=\"üí¨ ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°/‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á\")\n",
    "        btn = gr.Button(\"ü™Ñ ‡∏õ‡∏•‡∏î‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô\")\n",
    "        out = gr.Textbox(label=\"üì§ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\", lines=5) # ‡πÄ‡∏û‡∏¥‡πà‡∏° lines ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÑ‡∏î‡πâ‡πÄ‡∏¢‡∏≠‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "    btn.click(fn=chat, inputs=[txt, model_choice], outputs=out)\n",
    "\n",
    "    gr.Markdown(\"___\") # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á\n",
    "    gr.Markdown(\"### üé§ Whisper (‡∏û‡∏π‡∏î‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á)\")\n",
    "    mic = gr.Audio(source=\"microphone\", type=\"filepath\", label=\"‡∏Å‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏π‡∏î\")\n",
    "    txt_out = gr.Textbox(label=\"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "    mic.change(transcribe, inputs=mic, outputs=txt_out)\n",
    "\n",
    "    gr.Markdown(\"___\") # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á\n",
    "    gr.Markdown(\"### üé® ‡∏ß‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\")\n",
    "    prompt_img = gr.Textbox(label=\"Prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)\", placeholder=\"A cat in space, digital art\") # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ã‡πâ‡∏≥\n",
    "    btn_img = gr.Button(\"üñºÔ∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û\") # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û\n",
    "    img_out = gr.Image(label=\"‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á\")\n",
    "    btn_img.click(generate_image, inputs=prompt_img, outputs=img_out)\n",
    "\n",
    "\n",
    "print(\"Launching Gradio UI...\")\n",
    "try:\n",
    "    demo.launch(share=True)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to launch Gradio UI: {e}\")\n",
    "    print(\"This might be due to network issues or resource limitations in Colab.\")"
   ]
  }
 ]
}
